---
title: "GRIP:  generative  robust  inference  and  perception  for  semantic  robot manipulation in adversarial environments"
collection: publications
permalink: /publications/grip
excerpt: 'We develop a two-stage object detection and pose estimation system that aims to combine relative strength of discriminative CNNs and generative inference methods to achieve robust estimation under adversarial environments.'
date: 2019-8-17
venue: 'IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)'
paperurl: 'https://arxiv.org/abs/1903.08352'
authors: 'Xiaotong Chen, **Rui Chen**, Zhiqiang Sui, Zhefan Ye, Yanqi Liu, R. Iris Bahar, Odest Chadwicke Jenkins'
video: 'https://www.youtube.com/embed/W0KvzMhIJxo'
image: 'grip.png'
archive_image: 'grip.png'
archive_venue: 'Xiaotong Chen, **Rui Chen**, Zhiqiang Sui, Zhefan Ye, Yanqi Liu, R. Iris Bahar, Odest Chadwicke Jenkins. In *IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (To Appear)*. 2019'
# citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
---
# Abstract
Recent advancements have led to a proliferation of machine learning systems used to assist humans in a wide range of tasks. However, we are still far from accurate, reliable, and resource-efficient operations of these systems. For robot perception, convolutional neural networks (CNNs) for object detection and pose estimation are recently coming into widespread use. However, neural networks are known to suffer overfitting during training process and are less robust within unseen conditions, which are especially vulnerable to adversarial scenarios. In this work, we propose Generative Robust Inference and Perception (GRIP) as a two-stage object detection and pose estimation system that aims to combine relative strengths of discriminative CNNs and generative inference methods to achieve robust estimation. Our results show that a second stage of sample-based generative inference is able to recover from false object detection by CNNs, and produce robust estimations in adversarial conditions. We demonstrate the efficacy of GRIP robustness through comparison with state-of-the-art learning-based pose estimators and pick-and-place manipulation in dark and cluttered environments.

<!-- [Download paper here](https://arxiv.org/abs/1903.08352) -->

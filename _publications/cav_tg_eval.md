---
title: "How to Evaluate Self-Driving Testing Ground? A Quantitative Approach"
collection: publications
permalink: /publication/cav_tg_eval
excerpt: 'This work evaluates the effectiveness of connected and automated vehicle testing grounds in terms of their capability to accomodate real-world driving scenarios, which are extracted from naturalistic driving data via Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM).'
date: 2019-9-17
venue: 'arXiv'
# paperurl: 'https://arxiv.org/abs/1909.07843'
authors: '**Rui Chen**, Mansur Arief, Weiyang Zhang, Ding Zhao'
image: 'scenario_fit_demo.gif'
# codeurl: 'https://github.com/ruichen-v/active_learning_for_rsirl'
# citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
---
# Abstract
Testing ground has been a critical component in testing and validation for Connected and Automated Vehicles, or CAV. Although quite a few world-class testing facilities have been under construction over the years, the evaluation of testing grounds themselves as testing approaches has rarely been studied. In this paper, we investigate the effectiveness of CAV testing grounds by its capability to recreate real-world traffic scenarios. We extract typical use cases from naturalistic driving events leveraging non-parametric Bayesian learning techniques. Then, we contribute to a generative sample-based optimization approach to assess the compatibility between traffic scenarios and testing ground road structure. We evaluate the effectiveness of our approach with three CAV testing facilities: Mcity, Almono (Uber ATG), and Kcity. Experiments show that our approach is effective in evaluating the capability of a given CAV testing ground to accommodate real-world driving scenarios.
 
<!-- [Download paper here](https://arxiv.org/abs/1909.07843) -->
